Project 2 Submission for HCIRA, Spring '23

Group Members :
Aravind S
Nathan Harris
Shashanka Bhat

a) Parse Dataset:
We have stored the downloaded xml data in src folder. The code for parsing the xml data is stored in 'load_data_mmg.py' file. We use ElementTree provided by python to parse the xml files. A dictionary is created where the 1st level keys are for each user. The value for each user key is again a dictionary with the gesture labels as keys and the corresponding value is an array of gesture objects for each instance of the gesture for that user. The gesture object is used from the previous part of the project which handles the labels and points data of the gestures. Each gesture data is preprocessed and stored in the data. For multistroke dataset, we parse through all stroke tags and add the corresponding stroke number in Multistroke strokeid variable.

b) $P Recognizer:
We used a newly written $P recognizer to process the gestures. For the offline recognition part, we have recognize_with_n_best method which returns a sorted list of all gestures candidates and their corresponding scores. The dataset for offline is 10 users containing 10 instances of each of the 16 gestures. For the online recognition we use the recognize method. The candidates for online recognition we use 1 of the users dataset and store only 1 instance of each gesture.

Major Changes in comparison to previous $1 recognizer :
  1.The class Point is a representation of a point in a two-dimensional coordinate system, which contains the coordinates of the point (x, y) and has an optional stroke ID added for the multistroke recognizer

  2.The "greedy cloud distance" method calculates the distance between two point clouds (i.e., sets of points in 2D space) by greedily pairing the points in one cloud to the closest points in the other cloud. This results in a sum of distances between pairs of points in the two clouds. The method is "greedy" because it makes the closest possible pairing at each step without considering future pairings.

  3.The "greedy cloud match" method uses the greedy cloud distance method to compare a user-drawn gesture to a set of templates. It works by calculating the distance between the user-drawn gesture and each template, and then returning the template with the lowest distance. 

c) Testing:
Iterating over the dataset and testing it occurs in 'test.py'. This file is entirely devoted to performing the testing and is largely unchanged from our implementation from Part 3. Helper functions for managing the dataframe objects that store test data and running the recognizers are imported, as well as the variable that holds all of that data after it has been processed from 'load_data.py'

d) Logfile Generation:
The utility methods for storing the output for each iteration and comverting it to csv is present in 'output_util.py'. An empty dataframe with column names is created in as a template in the output_util.py file. 'create_empty_dataframe()' can be used to create an empty from the main testing program. 'add_list_to_dataframe()' takes in an existing dataframe and a list of values as a parameter. This method adds the list to the exisiting datafrane and returns back the updated dataframe. 'convert_dataframe_to_csv()' takes in a dataframe and filelocation as a paramanter and writes the dataframe to a csv file and stores the file in the given location.


Files specific to $P implementation are:
1. load_data_mmg.py - Load, process and store $P XML dataset. (We use 1st 6 user's Medium speed dataset for the offline recognition part)
2. main-new.py - This file is similar to main.py file but used for $P recognition
3. preprocess_dollar_p.py - All preprocessing and recognition method for $P is written in this file)
4. multistroke.py - (Multistroke class file for stroing template points)

We have files from $1 project part as well as we reused a lot of code from that part.
The main resource used for this project was $P paper, where we used the pseudocode provided within the paper. Since most of the code is reused from $1 algorithm, the resources used for that part is also referenced.

The folder also contains 2 csv files, output files of $P algorithm for unistroke and multistroke dataset.
mmg folder contains all the dataset of multistroke gestures.
